{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Input.xlsx')\n",
    "stop_auditor = pd.read_csv('StopWords_Auditor.txt', names=[0])\n",
    "stop_currency = pd.read_csv('StopWords_Currencies.txt', delimiter='|', encoding='latin-1', names = ['currency','country'])\n",
    "stop_date_nums = pd.read_csv('StopWords_DatesandNumbers.txt', names=[0])\n",
    "stop_general = pd.read_csv('StopWords_Generic.txt', names=[0])\n",
    "stop_generic = pd.read_csv('StopWords_GenericLong.txt', names=[0])\n",
    "stop_geo = pd.read_csv('StopWords_Geographic.txt', names=[0])\n",
    "stop_names = pd.read_csv('StopWords_Names.txt', names=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "webpage = requests.get(data['URL'][1], headers=headers).text\n",
    "soup = BeautifulSoup(webpage, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in soup.find_all('p'):\n",
    "    words.append((i.text.split()))\n",
    "for i in soup.find_all('h1', class_ = 'entry-title'):\n",
    "    words.append(i.text.strip())\n",
    "\n",
    "new_words = []\n",
    "for i in range(len(words)):\n",
    "    for j in words[i]:\n",
    "        new_words.append(j)\n",
    "raw_words = new_words            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = (list(stop_auditor.values) + list(stop_currency.values) + list(stop_date_nums.values) + list(stop_general.values) + list(stop_generic.values) + list(stop_geo.values) + list(stop_names.values))\n",
    "stopwords = []\n",
    "for i in range(len(stopwords_list)):\n",
    "    stopwords.append(stopwords_list[i][0])\n",
    "new_words = ([word for word in new_words if word not in stopwords])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive-words.txt', encoding='latin-1', names = [0]).values\n",
    "negative = pd.read_csv('negative-words.txt', encoding='latin-1', names = [0]).values\n",
    "p_words = []\n",
    "n_words = []\n",
    "for i in range(len(positive)):\n",
    "    p_words.append(positive[i][0])\n",
    "    \n",
    "for i in range(len(negative)):\n",
    "    n_words.append(negative[i][0]) \n",
    "positive_words = ([word for word in p_words if word not in stopwords])\n",
    "negative_words = ([word for word in n_words if word not in stopwords])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = ' '.join(new_words)\n",
    "token_words = word_tokenize(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis (token_words):\n",
    "    count_positive = 0\n",
    "    count_negative = 0\n",
    "    for word in token_words:\n",
    "        if word in positive_words:\n",
    "            count_positive += 1\n",
    "        if word in negative_words:\n",
    "            count_negative += 1\n",
    "    subjectivity = ((count_positive + count_negative)/len(new_words)) + 0.000001\n",
    "    if (count_negative + count_positive) > 0 :\n",
    "        polarity = (count_positive - count_negative)/(count_negative + count_positive) + 0.000001  \n",
    "    else:\n",
    "        polarity = np.nan\n",
    "    return count_positive, count_negative, subjectivity, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 38, 0.01734097417450655, 0.19149036170212766)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    syllables = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        syllables += 1\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            syllables += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        syllables -= 1\n",
    "    if syllables == 0:\n",
    "        syllables += 1\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_syllables(token_words):\n",
    "    number_of_syllables = 0\n",
    "    for i in range(len(token_words)):\n",
    "        syllables = syllable_count(token_words[i])\n",
    "        number_of_syllables += syllables\n",
    "    return number_of_syllables    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_syllables(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_count(token_words):\n",
    "    complex_words = 0\n",
    "    for i in range(len(token_words)):\n",
    "        syllables = syllable_count(token_words[i])\n",
    "        if syllables > 2 :\n",
    "            complex_words +=1\n",
    "    return complex_words        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_word_count(token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_of_readibility(new_words):\n",
    "    token_sentence = sent_tokenize(new_words)\n",
    "    \n",
    "    complex_words = 0\n",
    "    for i in range(len(token_words)):\n",
    "        syllables = syllable_count(token_words[i])\n",
    "        if syllables > 2 :\n",
    "            complex_words +=1\n",
    "            \n",
    "    percent_complex_words = complex_words/len(token_words)       \n",
    "    avg_sentence_length = len(token_words)/len(token_sentence)\n",
    "    fog_index = (percent_complex_words + avg_sentence_length) * 0.4\n",
    "    return percent_complex_words, avg_sentence_length, fog_index\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2470076169749728, 11.4875, 4.69380304678999)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_of_readibility(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_words_per_sentence(token_words, token_sentence):\n",
    "    return (len(token_words)/len(token_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.4875"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sentence = sent_tokenize(new_words)\n",
    "avg_words_per_sentence(token_words, token_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(new_words):\n",
    "    nltk_stopwords = (nltk.corpus.stopwords.words('english'))\n",
    "    clean_words = ([word for word in new_words.split() if word not in nltk_stopwords])\n",
    "    clean_words = ' '.join(clean_words)\n",
    "    clean_words = [char for char in clean_words if char not in string.punctuation]\n",
    "    clean_words = ''.join(clean_words)\n",
    "    total_cleaned_words = len(clean_words.split())\n",
    "    return total_cleaned_words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronouns(raw_words):\n",
    "    personal_pronouns = ['I', 'we', 'my', 'ours', 'us', 'We', 'My', 'Ours', 'Us']\n",
    "    pronouns = []\n",
    "    for i in range(len(raw_words)):\n",
    "        if raw_words[i] in personal_pronouns:\n",
    "            pronouns.append(raw_words[i]) \n",
    "    return (len(pronouns))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_pronouns(raw_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(clean_words):\n",
    "    word_length_per_word = []\n",
    "    for i in range(len(clean_words.split())):\n",
    "        word = clean_words.split()[i]\n",
    "        word_length = 0\n",
    "        for j in range(len(word)):\n",
    "            word_length = word_length + 1\n",
    "        word_length_per_word.append(word_length)    \n",
    "    avg_word_length = sum(word_length_per_word)/len(clean_words.split())  \n",
    "    return avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5523255813953485"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    nltk_stopwords = (nltk.corpus.stopwords.words('english'))\n",
    "    clean_words = ([word for word in new_words.split() if word not in nltk_stopwords])\n",
    "    clean_words = ' '.join(clean_words)\n",
    "    clean_words = [char for char in clean_words if char not in string.punctuation]\n",
    "    clean_words = ''.join(clean_words)\n",
    "    avg_word_length(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "positive_score_ = []\n",
    "negative_score_ = []\n",
    "polarity_score_ = []\n",
    "subjectivity_score_ = []\n",
    "percent_complex_words_ = []\n",
    "avg_sentence_length_ = []\n",
    "fog_index_ = []\n",
    "avg_words_per_sentence_ = []\n",
    "complex_word_count_ = []\n",
    "word_count_ = []\n",
    "syllable_count_ = []\n",
    "personal_pronouns_ = []\n",
    "avg_word_length_ = []\n",
    "\n",
    "for i in range(len(data['URL'])):\n",
    "    print(i)\n",
    "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    webpage = requests.get(data['URL'][i], headers=headers).text\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    words = []\n",
    "    for i in soup.find_all('p'):\n",
    "        words.append((i.text.split()))\n",
    "    for i in soup.find_all('h1', class_ = 'entry-title'):\n",
    "        words.append(i.text.strip())\n",
    "\n",
    "    new_words = []\n",
    "    for i in range(len(words)):\n",
    "        for j in words[i]:\n",
    "            new_words.append(j)\n",
    "    raw_words = new_words  \n",
    "    new_words = ' '.join(new_words)\n",
    "    token_words = word_tokenize(new_words)\n",
    "    token_sentence = sent_tokenize(new_words)\n",
    "\n",
    "    nltk_stopwords = (nltk.corpus.stopwords.words('english'))\n",
    "    clean_words = ([word for word in new_words.split() if word not in nltk_stopwords])\n",
    "    clean_words = ' '.join(clean_words)\n",
    "    clean_words = [char for char in clean_words if char not in string.punctuation]\n",
    "    clean_words = ''.join(clean_words)\n",
    "    \n",
    "    if len(new_words) != 0:\n",
    "        positive, negative, polarity, subjectivity = sentiment_analysis(token_words)\n",
    "        positive_score_.append(positive)\n",
    "        negative_score_.append(negative)\n",
    "        polarity_score_.append(polarity)\n",
    "        subjectivity_score_.append(subjectivity)\n",
    "\n",
    "        complex_words, sentence_length, index = analysis_of_readibility(new_words)\n",
    "        percent_complex_words_.append(complex_words)\n",
    "        avg_sentence_length_.append(sentence_length)\n",
    "        fog_index_.append(index)\n",
    "\n",
    "        words_per_sentence = avg_words_per_sentence(token_words, token_sentence)\n",
    "        avg_words_per_sentence_.append(words_per_sentence)\n",
    "\n",
    "        complex_words = complex_word_count(new_words)\n",
    "        complex_word_count_.append(complex_words)\n",
    "\n",
    "        word_counts = word_count(new_words)\n",
    "        word_count_.append(word_counts)\n",
    "\n",
    "        number_of_syllables = total_syllables(token_words)\n",
    "        syllable_count_.append(number_of_syllables)\n",
    "\n",
    "        pronoun_count = personal_pronouns(raw_words)\n",
    "        personal_pronouns_.append(pronoun_count)\n",
    "\n",
    "        word_lnth = avg_word_length(clean_words)\n",
    "        avg_word_length_.append(word_lnth)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(data = [positive_score_, negative_score_, polarity_score_, subjectivity_score_, percent_complex_words_, \n",
    "                    avg_sentence_length_, fog_index_, avg_words_per_sentence_, complex_word_count_, word_count_,\n",
    "                     syllable_count_, personal_pronouns_, avg_word_length_\n",
    "                    ]).T\n",
    "final_df.columns = ['positive_score_', \n",
    "'negative_score_', \n",
    "'polarity_score_', \n",
    "'subjectivity_score_', \n",
    "'percent_complex_words_', \n",
    "'avg_sentence_length_', \n",
    "'fog_index_', \n",
    "'avg_words_per_sentence_', \n",
    "'complex_word_count_', \n",
    "'word_count_', \n",
    "'syllable_count_', \n",
    "'personal_pronouns_', \n",
    "'avg_word_length_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_score_</th>\n",
       "      <th>negative_score_</th>\n",
       "      <th>polarity_score_</th>\n",
       "      <th>subjectivity_score_</th>\n",
       "      <th>percent_complex_words_</th>\n",
       "      <th>avg_sentence_length_</th>\n",
       "      <th>fog_index_</th>\n",
       "      <th>avg_words_per_sentence_</th>\n",
       "      <th>complex_word_count_</th>\n",
       "      <th>word_count_</th>\n",
       "      <th>syllable_count_</th>\n",
       "      <th>personal_pronouns_</th>\n",
       "      <th>avg_word_length_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>0.312501</td>\n",
       "      <td>0.224248</td>\n",
       "      <td>26.697368</td>\n",
       "      <td>10.768647</td>\n",
       "      <td>26.697368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.984153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>0.191490</td>\n",
       "      <td>0.146794</td>\n",
       "      <td>20.862500</td>\n",
       "      <td>8.403718</td>\n",
       "      <td>20.862500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>2525.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.089728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.300001</td>\n",
       "      <td>0.208441</td>\n",
       "      <td>22.858824</td>\n",
       "      <td>9.226906</td>\n",
       "      <td>22.858824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>3367.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.888268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.400001</td>\n",
       "      <td>0.142146</td>\n",
       "      <td>19.652174</td>\n",
       "      <td>7.917728</td>\n",
       "      <td>19.652174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>2773.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.958121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.371430</td>\n",
       "      <td>0.166496</td>\n",
       "      <td>25.350649</td>\n",
       "      <td>10.206858</td>\n",
       "      <td>25.350649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.458373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>-0.102040</td>\n",
       "      <td>0.190809</td>\n",
       "      <td>20.428571</td>\n",
       "      <td>8.247752</td>\n",
       "      <td>20.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.747826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>36.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.531916</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>24.820000</td>\n",
       "      <td>9.998588</td>\n",
       "      <td>24.820000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.540115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>-0.257142</td>\n",
       "      <td>0.184723</td>\n",
       "      <td>19.439394</td>\n",
       "      <td>7.849647</td>\n",
       "      <td>19.439394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.486601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>0.806453</td>\n",
       "      <td>0.225610</td>\n",
       "      <td>25.230769</td>\n",
       "      <td>10.182552</td>\n",
       "      <td>25.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.877384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>-0.098591</td>\n",
       "      <td>0.174873</td>\n",
       "      <td>17.848485</td>\n",
       "      <td>7.209343</td>\n",
       "      <td>17.848485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.334884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     positive_score_  negative_score_  polarity_score_  subjectivity_score_  \\\n",
       "0               63.0             33.0         0.007910             0.312501   \n",
       "1               56.0             38.0         0.011069             0.191490   \n",
       "2               65.0             35.0         0.009012             0.300001   \n",
       "3               56.0             24.0         0.008426             0.400001   \n",
       "4               48.0             22.0         0.006617             0.371430   \n",
       "..               ...              ...              ...                  ...   \n",
       "106             22.0             27.0         0.008445            -0.102040   \n",
       "107             36.0             11.0         0.006782             0.531916   \n",
       "108             26.0             44.0         0.009998            -0.257142   \n",
       "109             28.0              3.0         0.008120             0.806453   \n",
       "110             32.0             39.0         0.011090            -0.098591   \n",
       "\n",
       "     percent_complex_words_  avg_sentence_length_  fog_index_  \\\n",
       "0                  0.224248             26.697368   10.768647   \n",
       "1                  0.146794             20.862500    8.403718   \n",
       "2                  0.208441             22.858824    9.226906   \n",
       "3                  0.142146             19.652174    7.917728   \n",
       "4                  0.166496             25.350649   10.206858   \n",
       "..                      ...                   ...         ...   \n",
       "106                0.190809             20.428571    8.247752   \n",
       "107                0.176471             24.820000    9.998588   \n",
       "108                0.184723             19.439394    7.849647   \n",
       "109                0.225610             25.230769   10.182552   \n",
       "110                0.174873             17.848485    7.209343   \n",
       "\n",
       "     avg_words_per_sentence_  complex_word_count_  word_count_  \\\n",
       "0                  26.697368                  0.0       1199.0   \n",
       "1                  20.862500                  0.0        847.0   \n",
       "2                  22.858824                  0.0       1074.0   \n",
       "3                  19.652174                  0.0        979.0   \n",
       "4                  25.350649                  0.0       1045.0   \n",
       "..                       ...                  ...          ...   \n",
       "106                20.428571                  0.0        575.0   \n",
       "107                24.820000                  0.0        698.0   \n",
       "108                19.439394                  0.0        709.0   \n",
       "109                25.230769                  0.0        367.0   \n",
       "110                17.848485                  0.0        645.0   \n",
       "\n",
       "     syllable_count_  personal_pronouns_  avg_word_length_  \n",
       "0             3613.0                 2.0          6.984153  \n",
       "1             2525.0                 6.0          6.089728  \n",
       "2             3367.0                 2.0          6.888268  \n",
       "3             2773.0                17.0          5.958121  \n",
       "4             3106.0                14.0          6.458373  \n",
       "..               ...                 ...               ...  \n",
       "106           1683.0                 9.0          6.747826  \n",
       "107           2020.0                 3.0          6.540115  \n",
       "108           2107.0                 2.0          6.486601  \n",
       "109           1138.0                 2.0          6.877384  \n",
       "110           1926.0                 8.0          6.334884  \n",
       "\n",
       "[111 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
